---
title: 'opticut: likelihood based optimal partitioning for indicator species analysis'
author: "Peter Solymos, Ermias T. Azeria"
date: "November 17, 2015"
output:
  pdf_document:
    keep_tex: no
    number_sections: yes
    toc: yes
---

# Introduction

General problem: find where species abundances are high vs. low in
a way which leads to optimal classification by maximizing the
contrast between the partitions.

Previous attempts: historical review, highlighting IndVal.

Issues with previous attempts:

* summary statistics & Monte Carlo randomization with p-values, no model,
* data types not always compatible with randomization (i.e. decimals),
* confounding effects to classification can impact power,
* assessing the ranking of partitions without inferential statements.

Goals: 

* describe a general and extensible approach that addresses the above limitations,
* implement a computationally efficient algorithm,
* tools for exploring the results (i.e. summaries, plots) in a object oriented framework.

# Theory

## The quest for optimal binary partitioning

$Y_{i}$'s are observations for a single species
from $n$ locations ($i = 1, ..., n$).
$g_{i}$'s are known discrete descriptors of the locations with
$K$ levels ($K > 2$). $z^{(m)}$ is a binary reclassification of $g$ taking
values (0, 1). The superscript $m = 1, ..., M$ indicates a possible combination of binary reclassification out of the total $M = 2^{K-1} - 1$ total combinations (excluding complements). See below for options for defining binary partitions.
There can also be other site descriptors denoted as $x_{ij}$
taking discrete or continuous values ($j = 1, ..., p$; number of predictors).

A suitable parametric model describe the
relationship between the observations and the site descriptors
through the probability density function 
$P(Y_{i} = y_{i} \mid z_{i}^{(m)}, x_{ij}, \theta)$ 
where $\theta$ is the vector of model parameters: 
$\theta = (\beta_{0}, \beta_{1}, \alpha_{1}, ..., \alpha_{p})$.
The choice of the parametric model depends on the nature of the 
observations. It can be Gaussian, Binomial, Poisson,
ordinal, Beta regression, or zero-inflated models, with a 
suitable link function ($f$) for the mean:
$f(\eta_{i}) = \beta_{0}^{(m)} + \beta_{1}^{(m)} z_{i}^{(m)} + \sum_{j=1}^{p} \alpha_{j}^{(m)} x_{ij}$.

$\widehat{\theta^{(m)}}$ is the maximum likelihood estimate (MLE) of the 
model parameters given the data and classification $m$, 
with corresponding log-likelihood value $l(\widehat{\theta^{(m)}}; y)$.
Finding MLEs for all $M$ candidate binary partitions
leads to a set of log-likelihood values. One can compare
the log-likelihood values to a null model (no binary partition is necessary)
where $\beta_{1} = 0$ leading to the MLE $\widehat{\theta^{(0)}}$
and corresponding log-likelihood value for the null model: 
$l(\widehat{\theta^{(0)}}; y)$.

The log-likelihood ratio for each candidate partition can be
calculated as $l(\widehat{\theta^{(m)}}; y) - l(\widehat{\theta^{(0)}}; y)$.
The best supported binary partition is
the model with the highest log-likelihood ratio value.

The indicator value ($I$) for each candidate partition can be calculated
based on expected values using the inverse link function as 
$\mu_{0}^{(m)} = f^{-1}(\beta_{0}^{(m)})$ and 
$\mu_{1}^{(m)} = f^{-1}(\beta_{0}^{(m)} + \beta_{1}^{(m)})$.
$I = 1 - min(\mu_{0}^{(m)}, \mu_{1}^{(m)}) / max(\mu_{0}^{(m)}, \mu_{1}^{(m)})$.
Where $\mu_{0}^{(m)} = E[Y_{i} \mid z_{i}^{(m)}=0, x_{ij}=0]$ and 
$\mu_{1}^{(m)} = E[Y_{i} \mid z_{i}^{(m)}=1, x_{ij}=0]$ are expected values
for the observations given the binary partition $z_{i}^{(m)}$
and at 0 value for all $x_{ij}$. 

## Finding all possible binary partitions

Finding all combinations does not require a model or observed responses.
It only takes a classification vector with $K > 1$ partitions.

`kComb` returns a 'contrast' matrix corresponding to
all possible binary partitions of the factor with `K` levels.
Complements are not counted twice, i.e.
(0,0,1,1) is equivalent to (1,1,0,0).
The number of such possible combinations is $M = 2^{K-1} - 1$.

Get the package and load it:
```{r}
#devtools::install_github("psolymos/opticut")
#devtools::install("~/repos/opticut")
#devtools::check("~/repos/opticut")
#devtools::build("~/repos/opticut", binary=TRUE)
library(opticut)
```

```{r}
kComb(k = 2)
kComb(k = 3)
kComb(k = 4)
```

`allComb` this takes a classification vector with at least 2 levels
and returns a model matrix with binary partitions. `checkComb`
checks if combinations are unique and non-complementary
(misfits are returned as attributes).

```{r}
(f <- rep(LETTERS[1:4], each=2))
(mc <- allComb(f, collapse = "_"))
checkComb(mc)
mc2 <- cbind(z = 1 - mc[,1], mc[,c(1:ncol(mc), 1)])
colnames(mc2) <- 1:ncol(mc2)
mc2
checkComb(mc2)
```

## Poisson count model example

```{r}
set.seed(1234)
n <- 200
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
table(x0,x1)
lam1 <- exp(0.5 + 0.5*x1 + -0.2*x2)
boxplot(lam1~x0)
Y1 <- rpois(n, lam1)
lam2 <- exp(0.1 + 0.5*ifelse(x0==4,1,0) + 0.2*x2)
boxplot(lam2~x0)
Y2 <- rpois(n, lam2)
lam3 <- exp(0.1 + -0.2*x2)
boxplot(lam3~x0)
Y3 <- rpois(n, lam3)
Y <- cbind(SPP1=Y1, SPP2=Y2, SPP3=Y3)
X <- model.matrix(~x2)
Z <- allComb(x0)
opticut1(Y1, X, Z, dist="poisson")
opticut1(Y2, X, Z, dist="poisson")
opticut1(Y3, X, Z, dist="poisson")
summary(m <- opticut(Y ~ x2, strata=x0, dist="poisson", comb="all"))
plot(m, cut=-Inf)
```

Describe here what is what in the output.

## Not using all possible partitions

Blindly fitting a model to all possible partitions is wasteful
use of resources. Instead, one can rank the $K$ partitions
based on expected response values 
($\mu_{1}, ..., \mu_{k}, ..., \mu_{K}$, 
where $\mu_{k}=E[Y_{i} \mid g_{i}=k, x_{ij}=0]$).
This way we have to explore only $K-1$ partitions:

```{r}
oComb(1:4)
```

`oComb` return the 'contrast' matrix based on the rank vector as input.
Rank 1 means lowest expected value among the partitions.

The function `rankComb` fits the model with multiple ($K > 2$) factor levels
to find out the ranking, and returns a binary classification matrix
similarly to `allComb`:

```{r}
head(rc <- rankComb(Y1, model.matrix(~x2), as.factor(x0), dist="poisson"))
attr(rc, "est")
```

Note that the ranking varies from species to species, thus
it is not possible to supply the resulting matrix as
`strata` definition:

```{r}
summary(opticut(Y ~ x2, strata=x0, dist="poisson", comb="rank"))
```

There is an overhead of fitting the model to calculate the ranking.
But computing efficiencies can be still high compared to all partitions
when the number of levels ($k$) is high.

# Distributions

Currently available distributions:

* `"gaussian"`: real valued continuous observations, e.g. biomass,
* `"poisson"`: Poisson count data,
* `"binomial"`: presence-absence type data,
* `"negbin"`: overdispersed Negative Binomial count data,
* `"beta"`: continuous response in the unit interval, e.g. percent cover,
* `"zip"`, `"zip2"`: zero-inflated Poisson counts (partitioning in count model:
  `"zip"`, or in zero model: `"zip2"`),
* `"zinb"`, `"zinb"`: zero-inflated Negative Binomial counts 
  (partitioning in count model: `"zinb"`, or in zero model: `"zinb2"`),
* `"ordered"`: response measured on ordinal scale, e.g. ordinal vegetation cover, 
* `"rsf"`, `"rspf"`: presence-only data using resource selection and resource selection 
  probability functions.


## Gaussian

```{r}
Y <- rnorm(n, log(lam1) + 10, 0.5)
(mod <- opticut(Y ~ x2, strata=x0, dist="gaussian"))
```

Legendre example

```{r}
gr <- rep(1:5, each=5)
spp <- cbind(Sp1=rep(c(4,6,5,3,2), each=5),
    Sp2=c(rep(c(8,4,6), each=5), 4,4,2, rep(0,7)),
    Sp3=rep(c(18,2,0,0,0), each=5))
rownames(spp) <- gr
spp
summary(mod <- opticut(spp ~ 1, strata=gr, dist="gaussian", comb="all"))
summary(mod <- opticut(spp ~ 1, strata=gr, dist="gaussian", comb="rank"))
```

```{r}
## DeCaceres & Legendre 2013 Oikos example from Fig 1
## Oikos 119: 1674-1684, 2010
## doi: 10.1111/j.1600-0706.2010.18334.x
Y <- c(0, 0, 3, 0, 2, 3, 0, 5, 5, 6, 3, 4)
z <- rep(1:3, each=4)
Z <- allComb(z)
Z <- cbind(Z, 1-Z)
colnames(Z) <- c("1", "2", "3", "2 3", "1 3", "1 2")
Z
try(opticut1(Y, Z=Z))
oc <- ocoptions(check_comb=FALSE, cut=-Inf) # relax the checks
opticut1(Y, Z=Z) # identical results for complementary partitions
ocoptions(oc) # restore defaults
```

BCI data

```{r, eval=FALSE}
library (vegan)
data (BCI)
library (BiodiversityR)  # available from R version 2.15.1, not older!
data (BCI.env)
BCI.soil <- read.delim ('http://www.davidzeleny.net/anadat-r/lib/exe/fetch.php?media=data:bci.soil.txt')
###
BCI.hab <- read.table("http://www.kharms.biology.lsu.edu/TORUS_Habitats.txt",
    sep="\t", header=TRUE)
```


## Binomial

```{r}
set.seed(1234)
n <- 1000
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
table(x0,x1)
p1 <- plogis(0.5 + 0.5*x1 + -0.2*x2)
boxplot(p1~x0)
Y1 <- rbinom(n, 1, p1)
p2 <- plogis(0.1 + 0.5*ifelse(x0==4,1,0) + 0.2*x2)
boxplot(p2~x0)
Y2 <- rbinom(n, 1, p2)
Y <- cbind(SPP1=Y1, SPP2=Y2)
X <- model.matrix(~x2)
Z <- allComb(x0)

summary(opticut(Y ~ x2, strata=x0, dist="binomial"))
```


## Poisson: Mite data set -- high performance computing

See computing time diffs and plotting options.

```{r}
library(vegan)
data(mite)
data(mite.env)
mite.env$hab <- with(mite.env, interaction(Shrub, Topo, drop=TRUE))
summary(mod0 <- opticut(as.matrix(mite) ~ SubsDens, mite.env,
    strata=mite.env$hab, dist="poisson", comb="all"))
plot(mod0)

system.time(aa <- opticut(as.matrix(mite) ~ 1, strata=mite.env$hab, dist="poisson", comb="rank"))
system.time(bb <- opticut(as.matrix(mite) ~ 1, strata=mite.env$hab, dist="poisson", comb="all"))

## sequential
system.time(opticut(as.matrix(mite) ~ 1, strata=mite.env$hab, dist="poisson"))
## parallel -- compare system times
library(parallel)
cl <- makeCluster(3)
system.time(opticut(as.matrix(mite) ~ 1, strata=mite.env$hab, dist="poisson", cl=cl))
stopCluster(cl)
## forking -- will not work on Windows
if (!.Platform$OS.type == "windows")
system.time(opticut(as.matrix(mite) ~ 1, strata=mite.env$hab, dist="poisson", cl=3))
```

## Percentages

### Dune data, cover type data as ordinal

See http://www.davidzeleny.net/anadat-r/doku.php/en:data:dune

```{r}
library(vegan)
data(dune)
data(dune.env)
dune.env$manure <- as.integer(dune.env$Manure) - 1
dune.env$moisture <- as.integer(dune.env$Moisture) - 1

oc <- ocoptions(collapse="+", cut=-Inf)

## ordinal regr
## (when nlevels() < 3 use logistic regression instead !!!
#Dune <- as.matrix(dune)
#Dune <- Dune[,apply(Dune, 2, function(z) length(unique(z)))>2]
#x1 <- opticut(Dune ~ 1, dune.env, strata=Management, dist="ordered")
#summary(x1)

#plot(x1, mar=c(5,5,3,3))

## Binarizing data
Dune01 <- ifelse(as.matrix(dune)>0,1,0)
x2 <- opticut(Dune01 ~ 1, strata=dune.env$Management, dist="binomial")
summary(x2)
plot(x2)
x3 <- opticut(Dune01 ~ manure + moisture, dune.env, strata=dune.env$Management, dist="binomial")
summary(x3)
plot(x3)

## Beta regression
Dune2 <- as.matrix(dune+0.5) / 10
x4 <- opticut(Dune2 ~ 1, strata=dune.env$Management, dist="beta")
summary(x4)
plot(x4)
x5 <- opticut(Dune2 ~ manure, dune.env, strata=dune.env$Management, dist="beta")
summary(x5)
plot(x5)

xx <- data.frame(#ord0=summary(x1)$summary$split,
    bin0=summary(x2)$summary$split,
    binx=summary(x3)$summary$split,
    bet0=summary(x4)$summary$split,
    betx=summary(x5)$summary$split)
rownames(xx) <- rownames(summary(x2)$summary)
xx
ocoptions(oc)
```

### Varespec data (% cover)

```{r}
library(vegan)
data(varespec)
data(varechem)
y <- as.matrix(varespec / 100)
range(y[y>0])
y[y <= 0] <- 0.0001
y <- y[,apply(y, 2, max) > 0.05]
varechem$grazing <- as.factor(ifelse(rownames(varechem) %in% c(5,6,7,8,13,14,15,16,
    18,19,20,22,23,24,26), "grazed", "ungrazed"))

x <- opticut(y ~ 1, varechem, strata=grazing, dist="beta")
summary(x)
plot(x, cut=-Inf)
```


Implement ZI-Beta (quite unreliable for such small data set)

```{r,eval=FALSE}
zi_beta_fun <- function(Y, X, linkinv, ...) {
    kx <- ncol(X)
    id1 <- Y > 0
    id0 <- !id1
    nll_ZIB_ML <- function(parms) {
        mu <- plogis(X %*% parms[1:kx])
        gamma <- exp(parms[kx + 1]) # precision
        phi <- plogis(parms[kx+2])
        alpha <- mu * gamma
        beta <- (1 - mu) * gamma
        loglik0 <- log(phi)
        loglik1 <- log(1 - phi) + suppressWarnings(dbeta(Y, 
            alpha, beta, log = TRUE))
        loglik <- sum(loglik0[id0]) + sum(loglik1[id1])
        if (!is.finite(loglik) || is.na(loglik))
            loglik <- -.Machine$double.xmax^(1/3)
        -loglik
    }
    Yv <- Y
    Yv[Y <= 0.001] <- 0.001
    ini <- c(coef(betareg::betareg(Yv ~ .-1, data=X)), zi=-5)
    X <- as.matrix(X)
    res <- optim(ini, nll_ZIB_ML, ...)
    list(coef=res$par,
        logLik=-res$value,
        linkinv=binomial("logit")$linkinv)
}
y <- as.matrix(varespec / 100)
range(y[y>0])
y <- y[,apply(y, 2, max) > 0.05]
zi_beta_fun(y[,3], data.frame(matrix(1, nrow(y), 1)))
opticut1(y[,1], matrix(1, nrow(y), 1), varechem$grazing, dist=zi_beta_fun)
```


### Stratigraphy example

```{r}
library(rioja)
data(aber)
strat.plot(aber$spec, aber$ages$Depth, scale.percent=TRUE, y.rev=TRUE)

z <- as.factor(cut(aber$ages$Depth, 5))
ab <- as.matrix(aber$spec) / 100
ab[ab == 0] <- 0.0001
ab <- ab[,apply(ab, 2, max) > 0.05]

a <- opticut(ab ~ 1, strata=z, comb="rank", dist="beta")
summary(a)
plot(a, sort=FALSE)
bp <- bestpart(a)

op <- par(mfrow=c(3,4), mar=c(2,2,1,1))
for (i in 1:12) {
    plot(ab[,i], aber$ages$Depth, type="l", ann=FALSE)
    segments(x0=rep(0, nrow(ab)), y0=aber$ages$Depth, x1=ab[,i], 
        col=ifelse(bp[,i] > 0, 2, 1))
    title(main=colnames(ab)[i])
}
par(op)
```


## Presence-only data

Describe RSF/RSPF differences especially related to covariates.

```{r}
## presence-only data
## single species model only:
## because the used distr is different for
## each species by definition.

library(ResourceSelection)
## settings
n.used <- 1000
m <- 10
n <- n.used * m
set.seed(1234)
x <- data.frame(x0=as.factor(sample(1:3, n, replace=TRUE)),
    x1=rnorm(n), x2=runif(n))
cfs <- c(1, -0.5, 0.1, -1, 0.5)
## Logistic RSPF model
dd <- simulateUsedAvail(x, cfs, n.used, m, link="logit")

Y <- dd$status
X <- model.matrix(~ x1 + x2, dd)
Z <- allComb(as.integer(dd$x0))

mod1 <- opticut(Y ~ x1 + x2, dd, strata=x0, dist="rsf", m=0, B=0)
mod1$species
mod2 <- opticut(Y ~ x1 + x2, dd, strata=x0, dist="rspf", m=0, B=0)
mod2$species
```

# Custom distributions

The `distr` argument accepts a function, so other parametric models
can be supplied which are avoided due to package dependencies. 

## Mixed models

Here is an example using mixed models and the package `lme4`:

```{r}
library(lme4)
set.seed(1234)
n <- 200
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
ee <- rnorm(n/5)
g <- rep(1:5, each=n/5)
lam1 <- exp(0.5 + 0.5*x1 + -0.2*x2 + ee[g])
Y1 <- rpois(n, lam1)

X <- model.matrix(~x2)
Z <- allComb(x0)

lmefun <- function(Y, X, linkinv, gr, ...) {
    X <- as.matrix(X)
    m <- glmer(Y ~ X-1 + (1|gr), family=poisson("log"), ...)
    list(coef=fixef(m),
        logLik=logLik(m),
        linkinv=family(m)$linkinv)
}
lmefun(Y1, X, gr=g)

opticut1(Y1, X, Z, dist=lmefun, gr=g)
```

## Imperfect detectability: N-mixture case

A single-visit based N-mixture is an example where
detection error is estimated. Let us compare results
based on naive GLM and N-mixture:

```{r}
library(detect)
set.seed(2345)
n <- 500
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
x3 <- runif(n, 0, 1)
lam <- exp(0.5 + 1*x1 + -0.2*x2)
p <- plogis(2 + -2*x3)
Y <- rpois(n, lam*p)

X <- model.matrix(~x2)

op <- par(mfrow=c(1,2))
boxplot((lam*p) ~ x0, ylab="lam*p", xlab="x0")
boxplot(lam ~ x0, ylab="lam", xlab="x0")
par(op)

svfun <- function(Y, X, linkinv, ...) {
    X <- as.matrix(X)
    m <- svabu(Y ~ X-1 | x3, ...)
    list(coef=coef(m, "sta"),
        logLik=logLik(m),
        linkinv=poisson()$linkinv)
}
svfun(Y, X)

## naive GLM
print(opticut1(Y, X, as.factor(x0), dist="poisson"), cut=-Inf)
## N-mixture
print(opticut1(Y, X, as.factor(x0), dist=svfun), cut=-Inf)
```

## Sampling differences: using offsets

Not accounting for unequal sampling effort can be quite misleading, especially
if that is related to habitat classes. This example shows how to take
advantage of the other arguments passed to the `...` in the `opticut`
function.

```{r}
set.seed(1234)
n <- 50
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
lam <- exp(0.5 + 1*x1 + -0.2*x2)
A <- ifelse(x0 %in% c(1,3), 1, 2)
Y <- rpois(n, lam*A)

op <- par(mfrow=c(1,2))
boxplot((lam*A) ~ x0, ylab="lam*A", xlab="x0")
boxplot(lam ~ x0, ylab="lam", xlab="x0")
par(op)

## no offset: incorrect
opticut(Y ~ x2, strata=x0, dist="poisson", comb="rank")$species
## with offsets: log Area
opticut(Y ~ x2, strata=x0, dist="poisson", offset=log(A), comb="rank")$species
```

## GAM models

```{r}
library(mgcv)
library(detect)
data(oven)
oven$veg <- factor(NA, c("agr","open","decid","conif", "mix"))
oven$veg[oven$pforest < 0.5] <- "open"
oven$veg[oven$pagri > 0.5 & oven$pforest < 0.5] <- "agr"
oven$veg[oven$pforest >= 0.5] <- "mix"
oven$veg[oven$pforest >= 0.5 & oven$pdecid >= 0.8] <- "decid"
oven$veg[oven$pforest >= 0.5 & oven$pdecid < 0.2] <- "conif"
table(oven$veg, useNA="always")
oven$xlat <- scale(oven$lat)
oven$xlong <- scale(oven$long)
gamfun <- function(Y, X, linkinv, Data, ...) {
    X <- as.matrix(X)
    m <- mgcv::gam(Y ~ X-1 + s(xlat) + s(xlong), Data, ...)
    list(coef=coef(m),
        logLik=logLik(m),
        linkinv=family(m)$linkinv)
}
x <- ifelse(oven$veg=="agr",1,0)
X <- model.matrix(~x)
gamfun(oven$count, X, Data=oven, family=poisson)
print(opticut1(oven$count, X=X[,1,drop=FALSE], oven$veg, dist=gamfun, 
    Data=oven, family=poisson), cut=-Inf)

o <- opticut(count ~ 1, oven, strata=veg, dist=gamfun, Data=oven, family=poisson)
summary(o)
o <- opticut(count ~ 1, oven, strata=veg, dist="poisson")
summary(o)
oven$pa <- ifelse(oven$count > 0, 1, 0)
o <- opticut(pa ~ 1, oven, strata=veg, dist=gamfun, Data=oven, family=binomial)
summary(o)
o <- opticut(pa ~ 1, oven, strata=veg, dist="binomial")
summary(o)
```

# Finding best partitions

It is useful to access the best binary partition

```{r}
set.seed(2345)
n <- 50
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
lam <- exp(0.5 + 1*x1 + -0.2*x2)
Y <- rpois(n, lam)
o <- opticut(Y ~ x2, strata=x0, dist="poisson", comb="rank")
summary(o)

bp <- bestpart(o)
head(bp)
```

The model based on the best partition can be returned as:

```{r}
bestmodel(o, which=1)
```

the `which` argument can be used to subset the species.

# Uncertainty

Uncertainty in $I$ values might be of interest.
The `type` argument for the `uncertainty` method can take the following values:

* `"asymp"`: asymptotic distribution of $I$, $\mu_{0}$ and $\mu_{1}$
  based on best partition found for the input object.
* `"boot"`: non-parametric bootstrap distribution of $I$, $\mu_{0}$ and $\mu_{1}$
  based on best partition found for the input object.
* `"multi"`: non-parametric bootstrap distribution of $I$, $\mu_{0}$ and $\mu_{1}$
  based on best partition found for the bootstrap data (i.e.
  the model ranking is re-evaluated each time).

```{r}
uc1 <- uncertainty(o, type="asymp", B=5000)
uc2 <- uncertainty(o, type="boot", B=200)
uc3 <- uncertainty(o, type="multi", B=200)

uc1$uncertainty[[1]]
uc2$uncertainty[[1]]
uc3$uncertainty[[1]]

plot(uc3$uncertainty[[1]])
wplot(uc3$uncertainty[[1]])

## performance comparisons for 10 species
YYY <- cbind(Y, Y, Y, Y, Y, Y, Y, Y, Y, Y)
colnames(YYY) <- LETTERS[1:10]
o <- opticut(YYY ~ x2, strata=x0, dist="poisson", comb="rank")

library(parallel)
cl <- makeCluster(2)
system.time(uncertainty(o, type="asymp", B=5000))
system.time(uncertainty(o, type="asymp", B=5000, cl=cl))

system.time(uncertainty(o, type="boot", B=100))
system.time(uncertainty(o, type="boot", B=100, cl=cl))

system.time(uncertainty(o, type="multi", B=100))
system.time(uncertainty(o, type="multi", B=100, cl=cl))
stopCluster(cl)
```




